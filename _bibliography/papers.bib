@article{bresler2026latent,
  title = {Rates of Estimation for Semi-supervised Learning with Latent Space Models*},
  author = {Bresler, Guy and Harbuzova, Alina and Zhou, Leqi},
  year = {2026},
  note = {In submission; manuscript available upon request.},
  selected = {true},
  abbr = {Preprint}
}

@article{wu2026koopman,
  title = {Least-Squares Multi-Step Koopman Operator Learning for Model Predictive Control},
  author = {Wu, Liang and Tan, Wallace Gian Yion and Zhou, Leqi and Braatz, Richard D. and Drgona, Jan},
  year = {2026},
  arxiv = {2601.11901},
  url = {https://arxiv.org/abs/2601.11901},
  abstract = {MPC is widely used in real-time applications, but practical implementations are typically restricted to convex QP formulations to ensure fast and certified execution. Koopman-based MPC enables QP-based control of nonlinear systems by lifting the dynamics to a higher-dimensional linear representation. However, existing approaches rely on single-step EDMD. Consequently, prediction errors may accumulate over long horizons when the EDMD operator is applied recursively. Moreover, the multi-step prediction loss is nonconvex with respect to the single-step EDMD operator, making long-horizon model identification particularly challenging. This paper proposes a multi-step EDMD framework that directly learns the condensed multi-step state-control mapping required for Koopman-MPC, thereby bypassing explicit identification of the lifted system matrices and subsequent model condensation. The resulting identification problem admits a convex least-squares formulation. We further show that the problem decomposes across prediction horizons and state coordinates, enabling parallel computation and row-wise -regularization for automatic dictionary pruning. A non-asymptotic finite-sample analysis demonstrates that, unlike one-step EDMD, the proposed method avoids error compounding and yields error bounds that depend only on the target multi-step mapping. Numerical examples validate improved long-horizon prediction accuracy and closed-loop performance.},
  selected = {true},
  abbr = {arXiv},
}

@article{huang2025shotgun,
  title = {Shotgun Assembly of Random Regular Graphs*},
  author = {Huang, Brice and Mossel, Elchanan and Sun, Nike and Zhang, Claire and Zhou, Leqi},
  year = {2025},
  arxiv = {1512.08473},
  url = {https://arxiv.org/abs/1512.08473},
  abstract = {Mossel and Ross (2019) introduce the shotgun assembly problem for random graphs: what radius $R$ ensures that the random graph $G$ can be uniquely recovered from its list of rooted $R$-neighborhoods, with high probability? Here we consider this question for random regular graphs of fixed degree $d\ge3$. A result of Bollob√°s (1982) implies efficient recovery at $R = (1 + \epsilon) \tfrac12 \log_{d-1}n$ with high probability --- moreover, this recovery algorithm uses only a summary of the distances in each neighborhood. We show that using the full neighborhood structure gives a sharper bound $$R = \frac{\log n + \log\log n}{2\log(d-1)} + O(1),$$ which we prove is tight up to the $O(1)$ term. One consequence of our proof is that if $G,H$ are independent graphs where $G$ follows the random regular law, then with high probability the graphs are non-isomorphic; furthermore, this can be efficiently certified by testing the $R$-neighborhood list of $H$ against the $R$-neighborhood of a single adversarially chosen vertex of $G$.},
  selected = {true},
  abbr = {arXiv},
}
